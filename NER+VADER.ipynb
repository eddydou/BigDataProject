{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6bb2e807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re, html\n",
    "import hashlib\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from gdeltdoc import GdeltDoc, Filters\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.types import CHAR, Float, Integer, JSON\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f4485dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "USER = \"root\"\n",
    "PWD  = \"2003\"\n",
    "HOST = \"127.0.0.1\"\n",
    "PORT = 3306\n",
    "DB   = \"NewsVader\"\n",
    "\n",
    "ENGINE_URL = f\"mysql+pymysql://{USER}:{PWD}@{HOST}:{PORT}/{DB}?charset=utf8mb4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1555b92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Base de donn√©es 'NewsVader' pr√™te\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CR√âATION DE LA BASE DE DONN√âES\n",
    "# ============================================================================\n",
    "def create_database():\n",
    "    \"\"\"Cr√©e la base de donn√©es si elle n'existe pas\"\"\"\n",
    "    ADMIN_URL = f\"mysql+pymysql://{USER}:{PWD}@{HOST}:{PORT}/?charset=utf8mb4\"\n",
    "    admin_engine = create_engine(ADMIN_URL, future=True, pool_pre_ping=True)\n",
    "    with admin_engine.begin() as conn:\n",
    "        conn.exec_driver_sql(f\"\"\"\n",
    "            CREATE DATABASE IF NOT EXISTS {DB}\n",
    "            CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci;\n",
    "        \"\"\")\n",
    "    print(f\" Base de donn√©es '{DB}' pr√™te\")\n",
    "\n",
    "create_database()\n",
    "engine = create_engine(ENGINE_URL, future=True, pool_pre_ping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6aefdf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Table 'articles' cr√©√©e/v√©rifi√©e\n",
      " Table 'daily_sentiment' cr√©√©e/v√©rifi√©e\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SCH√âMAS DES TABLES\n",
    "#  ============================================================================\n",
    "# \n",
    "# TABLE 1: Articles d√©taill√©s avec NER et sentiment\n",
    "DDL_ARTICLES = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS `articles` (\n",
    "  `id` BIGINT UNSIGNED AUTO_INCREMENT PRIMARY KEY,\n",
    "  `source` VARCHAR(255),\n",
    "  `title` TEXT,\n",
    "  `url` TEXT ,\n",
    "  `url_hash` CHAR(32),\n",
    "  `description` MEDIUMTEXT,\n",
    "  `content` MEDIUMTEXT,\n",
    "  `full_text` MEDIUMTEXT,\n",
    "  `published_date` DATETIME NULL,\n",
    "  `gdelt_date` DATETIME NULL,\n",
    "  `language` VARCHAR(16),\n",
    "  \n",
    "  -- Sentiment\n",
    "  `sentiment_compound` DOUBLE,\n",
    "  `sentiment_pos` DOUBLE,\n",
    "  `sentiment_neu` DOUBLE,\n",
    "  `sentiment_neg` DOUBLE,\n",
    "  `sentiment_label` VARCHAR(16),\n",
    "  \n",
    "  -- NER (stock√© en JSON)\n",
    "  `entities_persons` JSON,\n",
    "  `entities_organizations` JSON,\n",
    "  `entities_locations` JSON,\n",
    "  `entities_other` JSON,\n",
    "  `entities_count` INT DEFAULT 0,\n",
    "  \n",
    "  `created_at` TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "  `updated_at` TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n",
    "  \n",
    "  UNIQUE KEY `uk_url_hash` (`url_hash`),\n",
    "  KEY `idx_published_date` (`published_date`),\n",
    "  KEY `idx_gdelt_date` (`gdelt_date`),\n",
    "  KEY `idx_sentiment_label` (`sentiment_label`)\n",
    ") ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_general_ci;\n",
    "\"\"\"\n",
    "\n",
    "# TABLE 2: Agr√©gation quotidienne du sentiment\n",
    "DDL_DAILY_SENTIMENT = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS `daily_sentiment` (\n",
    "  `id` INT UNSIGNED AUTO_INCREMENT PRIMARY KEY,\n",
    "  `date` DATE NOT NULL,\n",
    "  `sentiment_mean` DOUBLE,\n",
    "  `sentiment_median` DOUBLE,\n",
    "  `sentiment_std` DOUBLE,\n",
    "  `articles_count` INT,\n",
    "  `positive_count` INT,\n",
    "  `neutral_count` INT,\n",
    "  `negative_count` INT,\n",
    "  `top_entities` JSON,\n",
    "  `updated_at` TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n",
    "  \n",
    "  UNIQUE KEY `uk_date` (`date`),\n",
    "  KEY `idx_date` (`date`)\n",
    ") ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_general_ci;\n",
    "\"\"\"\n",
    "\n",
    "def create_tables():\n",
    "    \"\"\"Cr√©e toutes les tables n√©cessaires\"\"\"\n",
    "    with engine.begin() as conn:\n",
    "        conn.exec_driver_sql(DDL_ARTICLES)\n",
    "        print(\" Table 'articles' cr√©√©e/v√©rifi√©e\")\n",
    "        \n",
    "        conn.exec_driver_sql(DDL_DAILY_SENTIMENT)\n",
    "        print(\" Table 'daily_sentiment' cr√©√©e/v√©rifi√©e\")\n",
    "\n",
    "create_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6ae242f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Chargement des mod√®les...\n",
      " Mod√®le spaCy charg√©\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# INITIALISATION DES ANALYSEURS\n",
    "# ============================================================================\n",
    "print(\"üîÑ Chargement des mod√®les...\")\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\" Mod√®le spaCy charg√©\")\n",
    "except:\n",
    "    print(\" Installez spaCy: python -m spacy download en_core_web_sm\")\n",
    "    nlp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d83ed61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FONCTIONS UTILITAIRES\n",
    "# ============================================================================\n",
    "\n",
    "def clean_text_soft(text: str) -> str:\n",
    "    \"\"\"Nettoie le texte HTML et URLs\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = html.unescape(text)\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text(\" \", strip=True)\n",
    "    text = re.sub(r'(https?://\\S+|www\\.\\S+)', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def label_from_compound(x: float) -> str:\n",
    "    \"\"\"Convertit score compound en label\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return \"Neutral\"\n",
    "    return \"Positive\" if x >= 0.05 else (\"Negative\" if x <= -0.05 else \"Neutral\")\n",
    "\n",
    "def generate_url_hash(url: str) -> str:\n",
    "    \"\"\"G√©n√®re un hash MD5 de l'URL\"\"\"\n",
    "    if not url:\n",
    "        return \"\"\n",
    "    return hashlib.md5(url.encode('utf-8')).hexdigest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d6b0d7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ANALYSE NER (Named Entity Recognition)\n",
    "# ============================================================================\n",
    "\n",
    "def extract_entities(text: str) -> dict:\n",
    "    \"\"\"Extrait les entit√©s nomm√©es avec spaCy\"\"\"\n",
    "    if not nlp or not text or not isinstance(text, str):\n",
    "        return {\n",
    "            \"persons\": [],\n",
    "            \"organizations\": [],\n",
    "            \"locations\": [],\n",
    "            \"other\": [],\n",
    "            \"count\": 0\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        doc = nlp(text[:100000])  # Limite √† 100k caract√®res pour la performance\n",
    "        \n",
    "        entities = {\n",
    "            \"persons\": [],\n",
    "            \"organizations\": [],\n",
    "            \"locations\": [],\n",
    "            \"other\": []\n",
    "        }\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            entity_text = ent.text.strip()\n",
    "            if len(entity_text) < 2:  # Ignore entit√©s trop courtes\n",
    "                continue\n",
    "                \n",
    "            if ent.label_ == \"PERSON\":\n",
    "                entities[\"persons\"].append(entity_text)\n",
    "            elif ent.label_ in [\"ORG\", \"PRODUCT\"]:\n",
    "                entities[\"organizations\"].append(entity_text)\n",
    "            elif ent.label_ in [\"GPE\", \"LOC\", \"FAC\"]:\n",
    "                entities[\"locations\"].append(entity_text)\n",
    "            else:\n",
    "                entities[\"other\"].append(entity_text)\n",
    "        \n",
    "        # D√©duplique et compte\n",
    "        for key in entities:\n",
    "            entities[key] = list(set(entities[key]))[:20]  # Max 20 par cat√©gorie\n",
    "        \n",
    "        entities[\"count\"] = sum(len(v) for v in entities.values())\n",
    "        return entities\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Erreur NER: {e}\")\n",
    "        return {\"persons\": [], \"organizations\": [], \"locations\": [], \"other\": [], \"count\": 0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2c2d01d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# R√âCUP√âRATION GDELT\n",
    "# ============================================================================\n",
    "\n",
    "def get_multiple_batches(num_batches=300):\n",
    "    \"\"\"R√©cup√®re les articles GDELT par lots\"\"\"\n",
    "    gd = GdeltDoc()\n",
    "    all_articles = []\n",
    "    \n",
    "    start_date = datetime(2017, 1, 1)\n",
    "    end_date = datetime(2025, 9, 20)\n",
    "    total_days = (end_date - start_date).days\n",
    "    days_per_batch = total_days // num_batches\n",
    "    current_date = start_date\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        if i == num_batches - 1:\n",
    "            period_end = end_date\n",
    "        else:\n",
    "            period_end = current_date + timedelta(days=days_per_batch)\n",
    "            \n",
    "        f = Filters(\n",
    "            start_date=current_date.strftime(\"%Y-%m-%d\"),\n",
    "            end_date=period_end.strftime(\"%Y-%m-%d\"),\n",
    "            num_records=250,\n",
    "            language=\"ENGLISH\"\n",
    "            #domain=[\"bloomberg.com\", \"theguardian.com\", \"ft.com\", \"economist.com\"]\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            df_batch = gd.article_search(f)\n",
    "            if not df_batch.empty:\n",
    "                all_articles.append(df_batch)\n",
    "                print(f\" Batch {i+1}/{num_batches} ({current_date.strftime('%Y-%m-%d')} √† {period_end.strftime('%Y-%m-%d')}): {len(df_batch)} articles\")\n",
    "        except Exception as e:\n",
    "            print(f\" Erreur batch {i+1}: {e}\")\n",
    "            \n",
    "        current_date = period_end\n",
    "        time.sleep(1)  # Rate limiting\n",
    "    \n",
    "    if all_articles:\n",
    "        final_df = pd.concat(all_articles, ignore_index=True)\n",
    "        final_df = final_df.drop_duplicates(subset=['url'], keep='first')\n",
    "        print(f\" Total: {len(final_df)} articles uniques\")\n",
    "        return final_df\n",
    "    return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "597ad595",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# ANALYSE COMPL√àTE (SENTIMENT + NER)\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_articles(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Applique sentiment + NER sur chaque article\"\"\"\n",
    "    if df.empty:\n",
    "        return df.copy()\n",
    "\n",
    "    print(\" Analyse des articles...\")\n",
    "    \n",
    "    # Extraction des colonnes\n",
    "    title = df.get(\"title\", pd.Series([\"\"]*len(df))).fillna(\"\")\n",
    "    content = df.get(\"content\", pd.Series([\"\"]*len(df))).fillna(\"\")\n",
    "    desc = df.get(\"description\", pd.Series([\"\"]*len(df))).fillna(\"\")\n",
    "    url = df.get(\"url\", df.get(\"DocumentIdentifier\", pd.Series([\"\"]*len(df)))).fillna(\"\")\n",
    "    source = df.get(\"domain\", df.get(\"sourceCommonName\", pd.Series([\"\"]*len(df)))).fillna(\"\")\n",
    "    lang = df.get(\"language\", pd.Series([\"\"]*len(df))).fillna(\"\")\n",
    "    \n",
    "    # Dates\n",
    "    published_raw = df.get(\"publishdate\", df.get(\"date\", pd.Series([None]*len(df))))\n",
    "    gdelt_raw = df.get(\"seendate\", pd.Series([None]*len(df)))\n",
    "    \n",
    "    # Parsing dates\n",
    "    pub_dt = pd.to_datetime(published_raw, errors=\"coerce\")\n",
    "    seen_dt = pd.to_datetime(gdelt_raw, errors=\"coerce\", format=\"%Y%m%dT%H%M%SZ\")\n",
    "    \n",
    "    # Nettoyage texte\n",
    "    full_text = (title + \" \" + content + \" \" + desc).map(clean_text_soft)\n",
    "    \n",
    "    # SENTIMENT\n",
    "    print(\" Analyse de sentiment...\")\n",
    "    scores = full_text.map(lambda t: analyzer.polarity_scores(t) if t else \n",
    "                           {\"compound\":0, \"pos\":0, \"neu\":1, \"neg\":0})\n",
    "    \n",
    "    # NER\n",
    "    print(\" Extraction des entit√©s nomm√©es...\")\n",
    "    entities = full_text.apply(extract_entities)\n",
    "    \n",
    "    # Construction du DataFrame final\n",
    "    out = pd.DataFrame({\n",
    "        \"source\": source.astype(str).str[:255],\n",
    "        \"url\": url.astype(str),\n",
    "        \"url_hash\": url.astype(str).map(generate_url_hash),\n",
    "        \"title\": title.astype(str),\n",
    "        \"description\": desc.astype(str),\n",
    "        \"content\": content.astype(str),\n",
    "        \"full_text\": full_text,\n",
    "        \"published_date\": pub_dt,\n",
    "        \"gdelt_date\": seen_dt,\n",
    "        \"language\": lang.astype(str).str[:16],\n",
    "        \n",
    "        # Sentiment\n",
    "        \"sentiment_compound\": scores.map(lambda s: s[\"compound\"]),\n",
    "        \"sentiment_pos\": scores.map(lambda s: s[\"pos\"]),\n",
    "        \"sentiment_neu\": scores.map(lambda s: s[\"neu\"]),\n",
    "        \"sentiment_neg\": scores.map(lambda s: s[\"neg\"]),\n",
    "        \"sentiment_label\": scores.map(lambda s: label_from_compound(s[\"compound\"])),\n",
    "        \n",
    "        # Entit√©s (JSON)\n",
    "        \"entities_persons\": entities.map(lambda e: json.dumps(e[\"persons\"])),\n",
    "        \"entities_organizations\": entities.map(lambda e: json.dumps(e[\"organizations\"])),\n",
    "        \"entities_locations\": entities.map(lambda e: json.dumps(e[\"locations\"])),\n",
    "        \"entities_other\": entities.map(lambda e: json.dumps(e[\"other\"])),\n",
    "        \"entities_count\": entities.map(lambda e: e[\"count\"])\n",
    "    })\n",
    "    \n",
    "    print(f\" Analyse termin√©e: {len(out)} articles trait√©s\")\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "202d1b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# INSERTION EN BASE\n",
    "# ============================================================================\n",
    "\n",
    "def upsert_articles(df_scored: pd.DataFrame):\n",
    "    \"\"\"Insert/Update dans la table articles\"\"\"\n",
    "    if df_scored.empty:\n",
    "        print(\"‚ö†Ô∏è Aucun article √† ins√©rer\")\n",
    "        return 0\n",
    "\n",
    "    # Conversion ligne par ligne pour g√©rer tous les types de NaN/NaT\n",
    "    payload = []\n",
    "    for _, row in df_scored.iterrows():\n",
    "        clean_row = {}\n",
    "        \n",
    "        # Colonnes texte\n",
    "        for col in ['source', 'url', 'url_hash', 'title', 'description', 'content', 'full_text', 'language',\n",
    "                    'entities_persons', 'entities_organizations', 'entities_locations', 'entities_other', 'sentiment_label']:\n",
    "            val = row.get(col)\n",
    "            if pd.isna(val) or val is pd.NaT:\n",
    "                clean_row[col] = None\n",
    "            else:\n",
    "                clean_row[col] = str(val) if val is not None else None\n",
    "        \n",
    "        # Colonnes dates - conversion explicite\n",
    "        for col in ['published_date', 'gdelt_date']:\n",
    "            val = row.get(col)\n",
    "            if pd.isna(val) or val is pd.NaT:\n",
    "                clean_row[col] = None\n",
    "            else:\n",
    "                try:\n",
    "                    # Conversion en datetime Python natif\n",
    "                    if hasattr(val, 'to_pydatetime'):\n",
    "                        clean_row[col] = val.to_pydatetime()\n",
    "                    else:\n",
    "                        clean_row[col] = val\n",
    "                except:\n",
    "                    clean_row[col] = None\n",
    "        \n",
    "        # Colonnes num√©riques\n",
    "        for col in ['sentiment_compound', 'sentiment_pos', 'sentiment_neu', 'sentiment_neg', 'entities_count']:\n",
    "            val = row.get(col)\n",
    "            if pd.isna(val):\n",
    "                clean_row[col] = 0.0 if col != 'entities_count' else 0\n",
    "            else:\n",
    "                clean_row[col] = float(val) if col != 'entities_count' else int(val)\n",
    "        \n",
    "        payload.append(clean_row)\n",
    "    \n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            sql = text(\"\"\"\n",
    "                INSERT INTO articles\n",
    "                  (source, url, url_hash, title, description, content, full_text,\n",
    "                   published_date, gdelt_date, language,\n",
    "                   sentiment_compound, sentiment_pos, sentiment_neu, sentiment_neg, sentiment_label,\n",
    "                   entities_persons, entities_organizations, entities_locations, entities_other, entities_count)\n",
    "                VALUES\n",
    "                  (:source, :url, :url_hash, :title, :description, :content, :full_text,\n",
    "                   :published_date, :gdelt_date, :language,\n",
    "                   :sentiment_compound, :sentiment_pos, :sentiment_neu, :sentiment_neg, :sentiment_label,\n",
    "                   :entities_persons, :entities_organizations, :entities_locations, :entities_other, :entities_count)\n",
    "                ON DUPLICATE KEY UPDATE\n",
    "                  title=VALUES(title),\n",
    "                  description=VALUES(description),\n",
    "                  content=VALUES(content),\n",
    "                  full_text=VALUES(full_text),\n",
    "                  published_date=VALUES(published_date),\n",
    "                  gdelt_date=VALUES(gdelt_date),\n",
    "                  sentiment_compound=VALUES(sentiment_compound),\n",
    "                  sentiment_pos=VALUES(sentiment_pos),\n",
    "                  sentiment_neu=VALUES(sentiment_neu),\n",
    "                  sentiment_neg=VALUES(sentiment_neg),\n",
    "                  sentiment_label=VALUES(sentiment_label),\n",
    "                  entities_persons=VALUES(entities_persons),\n",
    "                  entities_organizations=VALUES(entities_organizations),\n",
    "                  entities_locations=VALUES(entities_locations),\n",
    "                  entities_other=VALUES(entities_other),\n",
    "                  entities_count=VALUES(entities_count)\n",
    "            \"\"\")\n",
    "            \n",
    "            conn.execute(sql, payload)\n",
    "        \n",
    "        print(f\"‚úÖ {len(payload)} articles ins√©r√©s/mis √† jour dans 'articles'\")\n",
    "        return len(payload)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur insertion: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9f4151af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AGR√âGATION QUOTIDIENNE\n",
    "# ============================================================================\n",
    "\n",
    "def compute_daily_sentiment():\n",
    "    \"\"\"Calcule et stocke les statistiques quotidiennes\"\"\"\n",
    "    print(\"üìä Calcul des agr√©gations quotidiennes...\")\n",
    "    \n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            DATE(gdelt_date) as date,\n",
    "            AVG(sentiment_compound) as sentiment_mean,\n",
    "            STD(sentiment_compound) as sentiment_std,\n",
    "            COUNT(*) as articles_count,\n",
    "            SUM(CASE WHEN sentiment_label = 'Positive' THEN 1 ELSE 0 END) as positive_count,\n",
    "            SUM(CASE WHEN sentiment_label = 'Neutral' THEN 1 ELSE 0 END) as neutral_count,\n",
    "            SUM(CASE WHEN sentiment_label = 'Negative' THEN 1 ELSE 0 END) as negative_count,\n",
    "            GROUP_CONCAT(DISTINCT entities_persons) as all_persons,\n",
    "            GROUP_CONCAT(DISTINCT entities_organizations) as all_orgs,\n",
    "            GROUP_CONCAT(DISTINCT entities_locations) as all_locs\n",
    "        FROM articles\n",
    "        WHERE gdelt_date IS NOT NULL\n",
    "        GROUP BY DATE(gdelt_date)\n",
    "        ORDER BY date\n",
    "    \"\"\"\n",
    "    \n",
    "    df_daily = pd.read_sql(query, engine)\n",
    "    \n",
    "    # Calcul de la m√©diane (pas disponible en SQL standard)\n",
    "    median_query = \"\"\"\n",
    "        SELECT DATE(gdelt_date) as date, sentiment_compound\n",
    "        FROM articles\n",
    "        WHERE gdelt_date IS NOT NULL\n",
    "        ORDER BY gdelt_date, sentiment_compound\n",
    "    \"\"\"\n",
    "    df_for_median = pd.read_sql(median_query, engine)\n",
    "    medians = df_for_median.groupby('date')['sentiment_compound'].median().reset_index()\n",
    "    medians.columns = ['date', 'sentiment_median']\n",
    "    \n",
    "    df_daily = df_daily.merge(medians, on='date', how='left')\n",
    "    \n",
    "    # Top entit√©s par jour (simplifi√© - √† am√©liorer si besoin)\n",
    "    df_daily['top_entities'] = df_daily.apply(\n",
    "        lambda row: json.dumps({\n",
    "            \"persons\": [],\n",
    "            \"organizations\": [],\n",
    "            \"locations\": []\n",
    "        }), axis=1\n",
    "    )\n",
    "    \n",
    "    # Insertion dans daily_sentiment\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            for _, row in df_daily.iterrows():\n",
    "                sql = text(\"\"\"\n",
    "                    INSERT INTO daily_sentiment\n",
    "                      (date, sentiment_mean, sentiment_median, sentiment_std, \n",
    "                       articles_count, positive_count, neutral_count, negative_count, top_entities)\n",
    "                    VALUES\n",
    "                      (:date, :sentiment_mean, :sentiment_median, :sentiment_std,\n",
    "                       :articles_count, :positive_count, :neutral_count, :negative_count, :top_entities)\n",
    "                    ON DUPLICATE KEY UPDATE\n",
    "                      sentiment_mean=VALUES(sentiment_mean),\n",
    "                      sentiment_median=VALUES(sentiment_median),\n",
    "                      sentiment_std=VALUES(sentiment_std),\n",
    "                      articles_count=VALUES(articles_count),\n",
    "                      positive_count=VALUES(positive_count),\n",
    "                      neutral_count=VALUES(neutral_count),\n",
    "                      negative_count=VALUES(negative_count),\n",
    "                      top_entities=VALUES(top_entities)\n",
    "                \"\"\")\n",
    "                conn.execute(sql, row.to_dict())\n",
    "        \n",
    "        print(f\"‚úÖ {len(df_daily)} jours agr√©g√©s dans 'daily_sentiment'\")\n",
    "        return df_daily\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur agr√©gation: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c2afd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üöÄ D√âMARRAGE DE L'ANALYSE GDELT\n",
      "======================================================================\n",
      "\n",
      "üì° R√©cup√©ration des articles GDELT...\n",
      " Batch 1/6 (2017-01-01 √† 2018-06-15): 250 articles\n",
      " Batch 2/6 (2018-06-15 √† 2019-11-27): 250 articles\n",
      " Batch 3/6 (2019-11-27 √† 2021-05-10): 250 articles\n",
      " Batch 4/6 (2021-05-10 √† 2022-10-22): 250 articles\n",
      " Batch 5/6 (2022-10-22 √† 2024-04-04): 250 articles\n",
      " Batch 6/6 (2024-04-04 √† 2025-09-20): 250 articles\n",
      " Total: 1499 articles uniques\n",
      " Analyse des articles...\n",
      " Analyse de sentiment...\n",
      " Extraction des entit√©s nomm√©es...\n",
      " Analyse termin√©e: 1500 articles trait√©s\n",
      "‚úÖ 1500 articles ins√©r√©s/mis √† jour dans 'articles'\n",
      "üìä Calcul des agr√©gations quotidiennes...\n",
      "‚úÖ 178 jours agr√©g√©s dans 'daily_sentiment'\n",
      "\n",
      "======================================================================\n",
      "üìä R√âSUM√â DES R√âSULTATS\n",
      "======================================================================\n",
      "\n",
      "üì∞ Articles analys√©s: 1500\n",
      "üìÖ Jours couverts: 178\n",
      "\n",
      "üéØ Distribution sentiment:\n",
      "sentiment_label\n",
      "Neutral     954\n",
      "Positive    284\n",
      "Negative    262\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üè∑Ô∏è Entit√©s extraites (moyenne):\n",
      "  - Personnes: 1.2 par article\n",
      "\n",
      "‚úÖ Donn√©es disponibles dans MariaDB:\n",
      "   - Base: NewsVader\n",
      "   - Table 1: articles (d√©tails + NER + sentiment)\n",
      "   - Table 2: daily_sentiment (agr√©gation quotidienne)\n",
      "   - URL: mysql+pymysql://root:2003@127.0.0.1:3306/NewsVader?charset=utf8mb4\n",
      "\n",
      "======================================================================\n",
      "‚ú® TERMIN√â AVEC SUCC√àS!\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üöÄ D√âMARRAGE DE L'ANALYSE GDELT\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# 1. R√©cup√©ration des articles\n",
    "print(\"üì° R√©cup√©ration des articles GDELT...\")\n",
    "df = get_multiple_batches(num_batches=6)\n",
    "\n",
    "if df.empty:\n",
    "    print(\"‚ùå Aucun article r√©cup√©r√©\")\n",
    "else:\n",
    "    # 2. Analyse (Sentiment + NER)\n",
    "    scored = analyze_articles(df)\n",
    "\n",
    "    # 3. Insertion dans la table articles\n",
    "    upsert_articles(scored)\n",
    "\n",
    "    # 4. Agr√©gation quotidienne\n",
    "    daily = compute_daily_sentiment()\n",
    "\n",
    "    # 5. Aper√ßu des r√©sultats\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä R√âSUM√â DES R√âSULTATS\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    print(f\"\\nüì∞ Articles analys√©s: {len(scored)}\")\n",
    "    print(f\"üìÖ Jours couverts: {len(daily)}\")\n",
    "\n",
    "    print(\"\\nüéØ Distribution sentiment:\")\n",
    "    print(scored['sentiment_label'].value_counts())\n",
    "\n",
    "    print(\"\\nüè∑Ô∏è Entit√©s extraites (moyenne):\")\n",
    "    print(f\"  - Personnes: {scored['entities_count'].mean():.1f} par article\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Donn√©es disponibles dans MariaDB:\")\n",
    "    print(f\"   - Base: {DB}\")\n",
    "    print(f\"   - Table 1: articles (d√©tails + NER + sentiment)\")\n",
    "    print(f\"   - Table 2: daily_sentiment (agr√©gation quotidienne)\")\n",
    "    print(f\"   - URL: {ENGINE_URL}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" TERMIN√â AVEC SUCC√àS!\")\n",
    "    print(\"=\"*70 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
