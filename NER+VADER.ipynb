{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bb2e807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re, html\n",
    "import hashlib\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from gdeltdoc import GdeltDoc, Filters\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.types import CHAR, Float, Integer, JSON\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4485dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "USER = \"root\"\n",
    "PWD  = \"2003\"\n",
    "HOST = \"127.0.0.1\"\n",
    "PORT = 3306\n",
    "DB   = \"NewsVader\"\n",
    "\n",
    "ENGINE_URL = f\"mysql+pymysql://{USER}:{PWD}@{HOST}:{PORT}/{DB}?charset=utf8mb4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1555b92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Base de données 'NewsVader' prête\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CRÉATION DE LA BASE DE DONNÉES\n",
    "# ============================================================================\n",
    "def create_database():\n",
    "    \"\"\"Crée la base de données si elle n'existe pas\"\"\"\n",
    "    ADMIN_URL = f\"mysql+pymysql://{USER}:{PWD}@{HOST}:{PORT}/?charset=utf8mb4\"\n",
    "    admin_engine = create_engine(ADMIN_URL, future=True, pool_pre_ping=True)\n",
    "    with admin_engine.begin() as conn:\n",
    "        conn.exec_driver_sql(f\"\"\"\n",
    "            CREATE DATABASE IF NOT EXISTS {DB}\n",
    "            CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci;\n",
    "        \"\"\")\n",
    "    print(f\" Base de données '{DB}' prête\")\n",
    "\n",
    "create_database()\n",
    "engine = create_engine(ENGINE_URL, future=True, pool_pre_ping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6aefdf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Table 'articles' créée/vérifiée\n",
      " Table 'daily_sentiment' créée/vérifiée\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SCHÉMAS DES TABLES\n",
    "#  ============================================================================\n",
    "# \n",
    "# TABLE 1: Articles détaillés avec NER et sentiment\n",
    "DDL_ARTICLES = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS `articles` (\n",
    "  `id` BIGINT UNSIGNED AUTO_INCREMENT PRIMARY KEY,\n",
    "  `source` VARCHAR(255),\n",
    "  `title` TEXT,\n",
    "  `url` TEXT ,\n",
    "  `url_hash` CHAR(32),\n",
    "  `description` MEDIUMTEXT,\n",
    "  `content` MEDIUMTEXT,\n",
    "  `full_text` MEDIUMTEXT,\n",
    "  `published_date` DATETIME NULL,\n",
    "  `gdelt_date` DATETIME NULL,\n",
    "  `language` VARCHAR(16),\n",
    "  \n",
    "  -- Sentiment\n",
    "  `sentiment_compound` DOUBLE,\n",
    "  `sentiment_pos` DOUBLE,\n",
    "  `sentiment_neu` DOUBLE,\n",
    "  `sentiment_neg` DOUBLE,\n",
    "  `sentiment_label` VARCHAR(16),\n",
    "  \n",
    "  -- NER (stocké en JSON)\n",
    "  `entities_persons` JSON,\n",
    "  `entities_organizations` JSON,\n",
    "  `entities_locations` JSON,\n",
    "  `entities_other` JSON,\n",
    "  `entities_count` INT DEFAULT 0,\n",
    "  \n",
    "  `created_at` TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "  `updated_at` TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n",
    "  \n",
    "  UNIQUE KEY `uk_url_hash` (`url_hash`),\n",
    "  KEY `idx_published_date` (`published_date`),\n",
    "  KEY `idx_gdelt_date` (`gdelt_date`),\n",
    "  KEY `idx_sentiment_label` (`sentiment_label`)\n",
    ") ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_general_ci;\n",
    "\"\"\"\n",
    "\n",
    "# TABLE 2: Agrégation quotidienne du sentiment\n",
    "DDL_DAILY_SENTIMENT = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS `daily_sentiment` (\n",
    "  `id` INT UNSIGNED AUTO_INCREMENT PRIMARY KEY,\n",
    "  `date` DATE NOT NULL,\n",
    "  `sentiment_mean` DOUBLE,\n",
    "  `sentiment_median` DOUBLE,\n",
    "  `sentiment_std` DOUBLE,\n",
    "  `articles_count` INT,\n",
    "  `positive_count` INT,\n",
    "  `neutral_count` INT,\n",
    "  `negative_count` INT,\n",
    "  `top_entities` JSON,\n",
    "  `updated_at` TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n",
    "  \n",
    "  UNIQUE KEY `uk_date` (`date`),\n",
    "  KEY `idx_date` (`date`)\n",
    ") ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_general_ci;\n",
    "\"\"\"\n",
    "\n",
    "def create_tables():\n",
    "    \"\"\"Crée toutes les tables nécessaires\"\"\"\n",
    "    with engine.begin() as conn:\n",
    "        conn.exec_driver_sql(DDL_ARTICLES)\n",
    "        print(\" Table 'articles' créée/vérifiée\")\n",
    "        \n",
    "        conn.exec_driver_sql(DDL_DAILY_SENTIMENT)\n",
    "        print(\" Table 'daily_sentiment' créée/vérifiée\")\n",
    "\n",
    "create_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ae242f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chargement des modèles...\n",
      " Modèle spaCy chargé\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# INITIALISATION DES ANALYSEURS\n",
    "# ============================================================================\n",
    "print(\" Chargement des modèles...\")\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\" Modèle spaCy chargé\")\n",
    "except:\n",
    "    print(\" Installez spaCy: python -m spacy download en_core_web_sm\")\n",
    "    nlp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d83ed61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FONCTIONS UTILITAIRES\n",
    "# ============================================================================\n",
    "\n",
    "def clean_text_soft(text: str) -> str:\n",
    "    \"\"\"Nettoie le texte HTML et URLs\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = html.unescape(text)\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text(\" \", strip=True)\n",
    "    text = re.sub(r'(https?://\\S+|www\\.\\S+)', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def label_from_compound(x: float) -> str:\n",
    "    \"\"\"Convertit score compound en label\"\"\"\n",
    "    if pd.isna(x):\n",
    "        return \"Neutral\"\n",
    "    return \"Positive\" if x >= 0.05 else (\"Negative\" if x <= -0.05 else \"Neutral\")\n",
    "\n",
    "def generate_url_hash(url: str) -> str:\n",
    "    \"\"\"Génère un hash MD5 de l'URL\"\"\"\n",
    "    if not url:\n",
    "        return \"\"\n",
    "    return hashlib.md5(url.encode('utf-8')).hexdigest()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6b0d7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ANALYSE NER (Named Entity Recognition)\n",
    "# ============================================================================\n",
    "\n",
    "def extract_entities(text: str) -> dict:\n",
    "    \"\"\"Extrait les entités nommées avec spaCy\"\"\"\n",
    "    if not nlp or not text or not isinstance(text, str):\n",
    "        return {\n",
    "            \"persons\": [],\n",
    "            \"organizations\": [],\n",
    "            \"locations\": [],\n",
    "            \"other\": [],\n",
    "            \"count\": 0\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        doc = nlp(text[:100000])  # Limite à 100k caractères pour la performance\n",
    "        \n",
    "        entities = {\n",
    "            \"persons\": [],\n",
    "            \"organizations\": [],\n",
    "            \"locations\": [],\n",
    "            \"other\": []\n",
    "        }\n",
    "        \n",
    "        for ent in doc.ents:\n",
    "            entity_text = ent.text.strip()\n",
    "            if len(entity_text) < 2:  # Ignore entités trop courtes\n",
    "                continue\n",
    "                \n",
    "            if ent.label_ == \"PERSON\":\n",
    "                entities[\"persons\"].append(entity_text)\n",
    "            elif ent.label_ in [\"ORG\", \"PRODUCT\"]:\n",
    "                entities[\"organizations\"].append(entity_text)\n",
    "            elif ent.label_ in [\"GPE\", \"LOC\", \"FAC\"]:\n",
    "                entities[\"locations\"].append(entity_text)\n",
    "            else:\n",
    "                entities[\"other\"].append(entity_text)\n",
    "        \n",
    "        # Déduplique et compte\n",
    "        for key in entities:\n",
    "            entities[key] = list(set(entities[key]))[:20]  # Max 20 par catégorie\n",
    "        \n",
    "        entities[\"count\"] = sum(len(v) for v in entities.values())\n",
    "        return entities\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Erreur NER: {e}\")\n",
    "        return {\"persons\": [], \"organizations\": [], \"locations\": [], \"other\": [], \"count\": 0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c2d01d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# RÉCUPÉRATION GDELT\n",
    "# ============================================================================\n",
    "\n",
    "def get_multiple_batches(num_batches=300):\n",
    "    \"\"\"Récupère les articles GDELT par lots\"\"\"\n",
    "    gd = GdeltDoc()\n",
    "    all_articles = []\n",
    "    \n",
    "    start_date = datetime(2017, 1, 1)\n",
    "    end_date = datetime(2025, 9, 20)\n",
    "    total_days = (end_date - start_date).days\n",
    "    days_per_batch = total_days // num_batches\n",
    "    current_date = start_date\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        if i == num_batches - 1:\n",
    "            period_end = end_date\n",
    "        else:\n",
    "            period_end = current_date + timedelta(days=days_per_batch)\n",
    "            \n",
    "        f = Filters(\n",
    "            start_date=current_date.strftime(\"%Y-%m-%d\"),\n",
    "            end_date=period_end.strftime(\"%Y-%m-%d\"),\n",
    "            num_records=250,\n",
    "            language=\"ENGLISH\"\n",
    "            #domain=[\"bloomberg.com\", \"theguardian.com\", \"ft.com\", \"economist.com\"]\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            df_batch = gd.article_search(f)\n",
    "            if not df_batch.empty:\n",
    "                all_articles.append(df_batch)\n",
    "                print(f\" Batch {i+1}/{num_batches} ({current_date.strftime('%Y-%m-%d')} à {period_end.strftime('%Y-%m-%d')}): {len(df_batch)} articles\")\n",
    "        except Exception as e:\n",
    "            print(f\" Erreur batch {i+1}: {e}\")\n",
    "            \n",
    "        current_date = period_end\n",
    "        time.sleep(1)  # Rate limiting\n",
    "    \n",
    "    if all_articles:\n",
    "        final_df = pd.concat(all_articles, ignore_index=True)\n",
    "        final_df = final_df.drop_duplicates(subset=['url'], keep='first')\n",
    "        print(f\" Total: {len(final_df)} articles uniques\")\n",
    "        return final_df\n",
    "    return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "597ad595",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# ANALYSE COMPLÈTE (SENTIMENT + NER)\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_articles(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Applique sentiment + NER sur chaque article\"\"\"\n",
    "    if df.empty:\n",
    "        return df.copy()\n",
    "\n",
    "    print(\" Analyse des articles...\")\n",
    "    \n",
    "    # Extraction des colonnes\n",
    "    title = df.get(\"title\", pd.Series([\"\"]*len(df))).fillna(\"\")\n",
    "    content = df.get(\"content\", pd.Series([\"\"]*len(df))).fillna(\"\")\n",
    "    desc = df.get(\"description\", pd.Series([\"\"]*len(df))).fillna(\"\")\n",
    "    url = df.get(\"url\", df.get(\"DocumentIdentifier\", pd.Series([\"\"]*len(df)))).fillna(\"\")\n",
    "    source = df.get(\"domain\", df.get(\"sourceCommonName\", pd.Series([\"\"]*len(df)))).fillna(\"\")\n",
    "    lang = df.get(\"language\", pd.Series([\"\"]*len(df))).fillna(\"\")\n",
    "    \n",
    "    # Dates\n",
    "    published_raw = df.get(\"publishdate\", df.get(\"date\", pd.Series([None]*len(df))))\n",
    "    gdelt_raw = df.get(\"seendate\", pd.Series([None]*len(df)))\n",
    "    \n",
    "    # Parsing dates\n",
    "    pub_dt = pd.to_datetime(published_raw, errors=\"coerce\")\n",
    "    seen_dt = pd.to_datetime(gdelt_raw, errors=\"coerce\", format=\"%Y%m%dT%H%M%SZ\")\n",
    "    \n",
    "    # Nettoyage texte\n",
    "    full_text = (title + \" \" + content + \" \" + desc).map(clean_text_soft)\n",
    "    \n",
    "    # SENTIMENT\n",
    "    print(\" Analyse de sentiment...\")\n",
    "    scores = full_text.map(lambda t: analyzer.polarity_scores(t) if t else \n",
    "                           {\"compound\":0, \"pos\":0, \"neu\":1, \"neg\":0})\n",
    "    \n",
    "    # NER\n",
    "    print(\" Extraction des entités nommées...\")\n",
    "    entities = full_text.apply(extract_entities)\n",
    "    \n",
    "    # Construction du DataFrame final\n",
    "    out = pd.DataFrame({\n",
    "        \"source\": source.astype(str).str[:255],\n",
    "        \"url\": url.astype(str),\n",
    "        \"url_hash\": url.astype(str).map(generate_url_hash),\n",
    "        \"title\": title.astype(str),\n",
    "        \"description\": desc.astype(str),\n",
    "        \"content\": content.astype(str),\n",
    "        \"full_text\": full_text,\n",
    "        \"published_date\": pub_dt,\n",
    "        \"gdelt_date\": seen_dt,\n",
    "        \"language\": lang.astype(str).str[:16],\n",
    "        \n",
    "        # Sentiment\n",
    "        \"sentiment_compound\": scores.map(lambda s: s[\"compound\"]),\n",
    "        \"sentiment_pos\": scores.map(lambda s: s[\"pos\"]),\n",
    "        \"sentiment_neu\": scores.map(lambda s: s[\"neu\"]),\n",
    "        \"sentiment_neg\": scores.map(lambda s: s[\"neg\"]),\n",
    "        \"sentiment_label\": scores.map(lambda s: label_from_compound(s[\"compound\"])),\n",
    "        \n",
    "        # Entités (JSON)\n",
    "        \"entities_persons\": entities.map(lambda e: json.dumps(e[\"persons\"])),\n",
    "        \"entities_organizations\": entities.map(lambda e: json.dumps(e[\"organizations\"])),\n",
    "        \"entities_locations\": entities.map(lambda e: json.dumps(e[\"locations\"])),\n",
    "        \"entities_other\": entities.map(lambda e: json.dumps(e[\"other\"])),\n",
    "        \"entities_count\": entities.map(lambda e: e[\"count\"])\n",
    "    })\n",
    "    \n",
    "    print(f\" Analyse terminée: {len(out)} articles traités\")\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "202d1b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# INSERTION EN BASE\n",
    "# ============================================================================\n",
    "\n",
    "def upsert_articles(df_scored: pd.DataFrame):\n",
    "    \"\"\"Insert/Update dans la table articles\"\"\"\n",
    "    if df_scored.empty:\n",
    "        print(\" Aucun article à insérer\")\n",
    "        return 0\n",
    "\n",
    "    # Conversion ligne par ligne pour gérer tous les types de NaN/NaT\n",
    "    payload = []\n",
    "    for _, row in df_scored.iterrows():\n",
    "        clean_row = {}\n",
    "        \n",
    "        # Colonnes texte\n",
    "        for col in ['source', 'url', 'url_hash', 'title', 'description', 'content', 'full_text', 'language',\n",
    "                    'entities_persons', 'entities_organizations', 'entities_locations', 'entities_other', 'sentiment_label']:\n",
    "            val = row.get(col)\n",
    "            if pd.isna(val) or val is pd.NaT:\n",
    "                clean_row[col] = None\n",
    "            else:\n",
    "                clean_row[col] = str(val) if val is not None else None\n",
    "        \n",
    "        # Colonnes dates - conversion explicite\n",
    "        for col in ['published_date', 'gdelt_date']:\n",
    "            val = row.get(col)\n",
    "            if pd.isna(val) or val is pd.NaT:\n",
    "                clean_row[col] = None\n",
    "            else:\n",
    "                try:\n",
    "                    # Conversion en datetime Python natif\n",
    "                    if hasattr(val, 'to_pydatetime'):\n",
    "                        clean_row[col] = val.to_pydatetime()\n",
    "                    else:\n",
    "                        clean_row[col] = val\n",
    "                except:\n",
    "                    clean_row[col] = None\n",
    "        \n",
    "        # Colonnes numériques\n",
    "        for col in ['sentiment_compound', 'sentiment_pos', 'sentiment_neu', 'sentiment_neg', 'entities_count']:\n",
    "            val = row.get(col)\n",
    "            if pd.isna(val):\n",
    "                clean_row[col] = 0.0 if col != 'entities_count' else 0\n",
    "            else:\n",
    "                clean_row[col] = float(val) if col != 'entities_count' else int(val)\n",
    "        \n",
    "        payload.append(clean_row)\n",
    "    \n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            sql = text(\"\"\"\n",
    "                INSERT INTO articles\n",
    "                  (source, url, url_hash, title, description, content, full_text,\n",
    "                   published_date, gdelt_date, language,\n",
    "                   sentiment_compound, sentiment_pos, sentiment_neu, sentiment_neg, sentiment_label,\n",
    "                   entities_persons, entities_organizations, entities_locations, entities_other, entities_count)\n",
    "                VALUES\n",
    "                  (:source, :url, :url_hash, :title, :description, :content, :full_text,\n",
    "                   :published_date, :gdelt_date, :language,\n",
    "                   :sentiment_compound, :sentiment_pos, :sentiment_neu, :sentiment_neg, :sentiment_label,\n",
    "                   :entities_persons, :entities_organizations, :entities_locations, :entities_other, :entities_count)\n",
    "                ON DUPLICATE KEY UPDATE\n",
    "                  title=VALUES(title),\n",
    "                  description=VALUES(description),\n",
    "                  content=VALUES(content),\n",
    "                  full_text=VALUES(full_text),\n",
    "                  published_date=VALUES(published_date),\n",
    "                  gdelt_date=VALUES(gdelt_date),\n",
    "                  sentiment_compound=VALUES(sentiment_compound),\n",
    "                  sentiment_pos=VALUES(sentiment_pos),\n",
    "                  sentiment_neu=VALUES(sentiment_neu),\n",
    "                  sentiment_neg=VALUES(sentiment_neg),\n",
    "                  sentiment_label=VALUES(sentiment_label),\n",
    "                  entities_persons=VALUES(entities_persons),\n",
    "                  entities_organizations=VALUES(entities_organizations),\n",
    "                  entities_locations=VALUES(entities_locations),\n",
    "                  entities_other=VALUES(entities_other),\n",
    "                  entities_count=VALUES(entities_count)\n",
    "            \"\"\")\n",
    "            \n",
    "            conn.execute(sql, payload)\n",
    "        \n",
    "        print(f\" {len(payload)} articles insérés/mis à jour dans 'articles'\")\n",
    "        return len(payload)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Erreur insertion: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f4151af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# AGRÉGATION QUOTIDIENNE\n",
    "# ============================================================================\n",
    "\n",
    "def compute_daily_sentiment():\n",
    "    \"\"\"Calcule et stocke les statistiques quotidiennes\"\"\"\n",
    "    print(\" Calcul des agrégations quotidiennes...\")\n",
    "    \n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            DATE(gdelt_date) as date,\n",
    "            AVG(sentiment_compound) as sentiment_mean,\n",
    "            STD(sentiment_compound) as sentiment_std,\n",
    "            COUNT(*) as articles_count,\n",
    "            SUM(CASE WHEN sentiment_label = 'Positive' THEN 1 ELSE 0 END) as positive_count,\n",
    "            SUM(CASE WHEN sentiment_label = 'Neutral' THEN 1 ELSE 0 END) as neutral_count,\n",
    "            SUM(CASE WHEN sentiment_label = 'Negative' THEN 1 ELSE 0 END) as negative_count,\n",
    "            GROUP_CONCAT(DISTINCT entities_persons) as all_persons,\n",
    "            GROUP_CONCAT(DISTINCT entities_organizations) as all_orgs,\n",
    "            GROUP_CONCAT(DISTINCT entities_locations) as all_locs\n",
    "        FROM articles\n",
    "        WHERE gdelt_date IS NOT NULL\n",
    "        GROUP BY DATE(gdelt_date)\n",
    "        ORDER BY date\n",
    "    \"\"\"\n",
    "    \n",
    "    df_daily = pd.read_sql(query, engine)\n",
    "    \n",
    "    # Calcul de la médiane (pas disponible en SQL standard)\n",
    "    median_query = \"\"\"\n",
    "        SELECT DATE(gdelt_date) as date, sentiment_compound\n",
    "        FROM articles\n",
    "        WHERE gdelt_date IS NOT NULL\n",
    "        ORDER BY gdelt_date, sentiment_compound\n",
    "    \"\"\"\n",
    "    df_for_median = pd.read_sql(median_query, engine)\n",
    "    medians = df_for_median.groupby('date')['sentiment_compound'].median().reset_index()\n",
    "    medians.columns = ['date', 'sentiment_median']\n",
    "    \n",
    "    df_daily = df_daily.merge(medians, on='date', how='left')\n",
    "    \n",
    "    # Top entités par jour (simplifié - à améliorer si besoin)\n",
    "    df_daily['top_entities'] = df_daily.apply(\n",
    "        lambda row: json.dumps({\n",
    "            \"persons\": [],\n",
    "            \"organizations\": [],\n",
    "            \"locations\": []\n",
    "        }), axis=1\n",
    "    )\n",
    "    \n",
    "    # Insertion dans daily_sentiment\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            for _, row in df_daily.iterrows():\n",
    "                sql = text(\"\"\"\n",
    "                    INSERT INTO daily_sentiment\n",
    "                      (date, sentiment_mean, sentiment_median, sentiment_std, \n",
    "                       articles_count, positive_count, neutral_count, negative_count, top_entities)\n",
    "                    VALUES\n",
    "                      (:date, :sentiment_mean, :sentiment_median, :sentiment_std,\n",
    "                       :articles_count, :positive_count, :neutral_count, :negative_count, :top_entities)\n",
    "                    ON DUPLICATE KEY UPDATE\n",
    "                      sentiment_mean=VALUES(sentiment_mean),\n",
    "                      sentiment_median=VALUES(sentiment_median),\n",
    "                      sentiment_std=VALUES(sentiment_std),\n",
    "                      articles_count=VALUES(articles_count),\n",
    "                      positive_count=VALUES(positive_count),\n",
    "                      neutral_count=VALUES(neutral_count),\n",
    "                      negative_count=VALUES(negative_count),\n",
    "                      top_entities=VALUES(top_entities)\n",
    "                \"\"\")\n",
    "                conn.execute(sql, row.to_dict())\n",
    "        \n",
    "        print(f\" {len(df_daily)} jours agrégés dans 'daily_sentiment'\")\n",
    "        return df_daily\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Erreur agrégation: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40c2afd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      " DÉMARRAGE DE L'ANALYSE GDELT\n",
      "======================================================================\n",
      "\n",
      " Récupération des articles GDELT...\n",
      " Batch 1/3 (2017-01-01 à 2019-11-28): 250 articles\n",
      " Batch 2/3 (2019-11-28 à 2022-10-24): 250 articles\n",
      " Batch 3/3 (2022-10-24 à 2025-09-20): 250 articles\n",
      " Total: 750 articles uniques\n",
      " Analyse des articles...\n",
      " Analyse de sentiment...\n",
      " Extraction des entités nommées...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m Aucun article récupéré\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# 2. Analyse (Sentiment + NER)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     scored = \u001b[43manalyze_articles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# 3. Insertion dans la table articles\u001b[39;00m\n\u001b[32m     16\u001b[39m     upsert_articles(scored)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36manalyze_articles\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# NER\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m Extraction des entités nommées...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m entities = \u001b[43mfull_text\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_entities\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Construction du DataFrame final\u001b[39;00m\n\u001b[32m     41\u001b[39m out = pd.DataFrame({\n\u001b[32m     42\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: source.astype(\u001b[38;5;28mstr\u001b[39m).str[:\u001b[32m255\u001b[39m],\n\u001b[32m     43\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33murl\u001b[39m\u001b[33m\"\u001b[39m: url.astype(\u001b[38;5;28mstr\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     65\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mentities_count\u001b[39m\u001b[33m\"\u001b[39m: entities.map(\u001b[38;5;28;01mlambda\u001b[39;00m e: e[\u001b[33m\"\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     66\u001b[39m })\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\series.py:4935\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4800\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4801\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4802\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4807\u001b[39m     **kwargs,\n\u001b[32m   4808\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4809\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4810\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4811\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4926\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4927\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4928\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4929\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4930\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4931\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4932\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4933\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4935\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mextract_entities\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m      9\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpersons\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m     10\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33morganizations\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0\u001b[39m\n\u001b[32m     14\u001b[39m     }\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     doc = \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m100000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Limite à 100k caractères pour la performance\u001b[39;00m\n\u001b[32m     19\u001b[39m     entities = {\n\u001b[32m     20\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpersons\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m     21\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33morganizations\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m     22\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlocations\u001b[39m\u001b[33m\"\u001b[39m: [],\n\u001b[32m     23\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mother\u001b[39m\u001b[33m\"\u001b[39m: []\n\u001b[32m     24\u001b[39m     }\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m ent \u001b[38;5;129;01min\u001b[39;00m doc.ents:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\spacy\\language.py:1053\u001b[39m, in \u001b[36mLanguage.__call__\u001b[39m\u001b[34m(self, text, disable, component_cfg)\u001b[39m\n\u001b[32m   1051\u001b[39m     error_handler = proc.get_error_handler()\n\u001b[32m   1052\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m     doc = \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcomponent_cfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1055\u001b[39m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[32m   1056\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors.E109.format(name=name)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:52\u001b[39m, in \u001b[36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\spacy\\pipeline\\tok2vec.py:121\u001b[39m, in \u001b[36mTok2Vec.predict\u001b[39m\u001b[34m(self, docs)\u001b[39m\n\u001b[32m    119\u001b[39m     width = \u001b[38;5;28mself\u001b[39m.model.get_dim(\u001b[33m\"\u001b[39m\u001b[33mnO\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m.model.ops.alloc((\u001b[32m0\u001b[39m, width)) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs]\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m tokvecs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokvecs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\thinc\\model.py:334\u001b[39m, in \u001b[36mModel.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) -> OutT:\n\u001b[32m    331\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[32m    332\u001b[39m \u001b[33;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[32m    333\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\thinc\\layers\\chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\thinc\\model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\thinc\\layers\\with_array.py:42\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, Xseq, is_train)\u001b[39m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model.layers[\u001b[32m0\u001b[39m](Xseq, is_train)\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], \u001b[43m_list_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\thinc\\layers\\with_array.py:77\u001b[39m, in \u001b[36m_list_forward\u001b[39m\u001b[34m(model, Xs, is_train)\u001b[39m\n\u001b[32m     75\u001b[39m lengths = NUMPY_OPS.asarray1i([\u001b[38;5;28mlen\u001b[39m(seq) \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m Xs])\n\u001b[32m     76\u001b[39m Xf = layer.ops.flatten(Xs, pad=pad)\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m Yf, get_dXf = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackprop\u001b[39m(dYs: ListXd) -> ListXd:\n\u001b[32m     80\u001b[39m     dYf = layer.ops.flatten(dYs, pad=pad)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\thinc\\model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\thinc\\layers\\chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\thinc\\model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\thinc\\layers\\residual.py:41\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     39\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m d_output + dX\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m Y, backprop_layer = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [X[i] + Y[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X))], backprop\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\thinc\\model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\thinc\\layers\\chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\thinc\\model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\thinc\\layers\\chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "    \u001b[31m[... skipping similar frames: Model.__call__ at line 310 (1 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\thinc\\layers\\chain.py:54\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     52\u001b[39m callbacks = []\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model.layers:\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     Y, inc_layer_grad = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     callbacks.append(inc_layer_grad)\n\u001b[32m     56\u001b[39m     X = Y\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\thinc\\model.py:310\u001b[39m, in \u001b[36mModel.__call__\u001b[39m\u001b[34m(self, X, is_train)\u001b[39m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) -> Tuple[OutT, Callable]:\n\u001b[32m    308\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[32m    309\u001b[39m \u001b[33;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\thinc\\layers\\maxout.py:52\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(model, X, is_train)\u001b[39m\n\u001b[32m     50\u001b[39m W = model.get_param(\u001b[33m\"\u001b[39m\u001b[33mW\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     51\u001b[39m W = model.ops.reshape2f(W, nO * nP, nI)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m Y = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgemm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans2\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m Y += model.ops.reshape1f(b, nO * nP)\n\u001b[32m     54\u001b[39m Z = model.ops.reshape3f(Y, Y.shape[\u001b[32m0\u001b[39m], nO, nP)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" DÉMARRAGE DE L'ANALYSE GDELT\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# 1. Récupération des articles  ICI POUR MODIFIER LES BATCHES\n",
    "print(\" Récupération des articles GDELT...\")\n",
    "df = get_multiple_batches(num_batches=3)\n",
    "\n",
    "if df.empty:\n",
    "    print(\" Aucun article récupéré\")\n",
    "else:\n",
    "    # 2. Analyse (Sentiment + NER)\n",
    "    scored = analyze_articles(df)\n",
    "\n",
    "    # 3. Insertion dans la table articles\n",
    "    upsert_articles(scored)\n",
    "\n",
    "    # 4. Agrégation quotidienne\n",
    "    daily = compute_daily_sentiment()\n",
    "\n",
    "    # 5. Aperçu des résultats\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" RÉSUMÉ DES RÉSULTATS\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    print(f\"Articles analysés: {len(scored)}\")\n",
    "    print(f\" Jours couverts: {len(daily)}\")\n",
    "\n",
    "    print(\"Distribution sentiment:\")\n",
    "    print(scored['sentiment_label'].value_counts())\n",
    "\n",
    "    print(\"Entités extraites (moyenne):\")\n",
    "    print(f\"  - Personnes: {scored['entities_count'].mean():.1f} par article\")\n",
    "\n",
    "    print(f\"Données disponibles dans MariaDB:\")\n",
    "    print(f\"   - Base: {DB}\")\n",
    "    print(f\"   - Table 1: articles (détails + NER + sentiment)\")\n",
    "    print(f\"   - Table 2: daily_sentiment (agrégation quotidienne)\")\n",
    "    print(f\"   - URL: {ENGINE_URL}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\" TERMINÉ AVEC SUCCÈS!\")\n",
    "    print(\"=\"*70 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
